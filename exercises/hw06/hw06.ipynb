{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 6 - Experiments on MNIST for 10-class Classification\n",
    "\n",
    "Please implement the following three functions:\n",
    "- MnistMLP() - Design a 2-layer MLP\n",
    "- MnistCNN() - Design a 2-layer CNN \n",
    "\n",
    "Please train the 2-layer MLP and CNN models on the Mnist dataset and print the training results for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/zhouyuyang/env/miniconda3/envs/dl/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import Compose,ToTensor,Normalize\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "TEST_BATCH_SIZE = 1000\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# dataloader for the dataset\n",
    "def get_dataloader(train,batch_size=BATCH_SIZE):\n",
    "    transform_fn = Compose([\n",
    "        ToTensor(),\n",
    "        Normalize(mean = (0.1307,),std = (0.3081,))\n",
    "        ]) \n",
    "    dataset = MNIST(root = './data',train = train,transform = transform_fn, download = True)\n",
    "    data_loader = DataLoader(dataset,batch_size = batch_size,shuffle = True)\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-lyer MLP \n",
    "class MnistMLP(nn.Module):\n",
    "    # Tip: write `def __init__(self)` and `def forward(self,input)`\n",
    "    def __init__(self):\n",
    "        super(MnistMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)  # Input size is 28x28 (MNIST image dimensions), output is 128\n",
    "        self.fc2 = nn.Linear(128, 10)       # Output size is 10 (for 10 classes in MNIST)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten the input image tensor (batch_size, 1, 28, 28) -> (batch_size, 784)\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        \n",
    "        # Pass through first fully connected layer with ReLU activation\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        # Pass through second fully connected layer (output layer)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)  # Use log_softmax for numerical stability in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-lyer CNN\n",
    "class MnistCNN(nn.Module):\n",
    "    # Tip: write `def __init__(self)` and `def forward(self,input)`\n",
    "    def __init__(self):\n",
    "        super(MnistCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)  # Input size from conv2d output, output size is 128\n",
    "        self.fc2 = nn.Linear(128, 10)          # Output size is 10 (for 10 classes in MNIST)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First convolutional layer with max pooling and ReLU activation\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        \n",
    "        # Second convolutional layer with max pooling and ReLU activation\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        \n",
    "        # Flatten the output of the last convolutional layer\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        \n",
    "        # First fully connected layer with ReLU activation\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        # Second fully connected layer (output layer)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)  # Use log_softmax for numerical stability in training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MnistMLP().to(device)\n",
    "optimizer = Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, num_epochs):\n",
    "    data_loader = get_dataloader(True)\n",
    "    total_step = len(data_loader)\n",
    "    for idx, (input, target) in enumerate(data_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input.to(device))\n",
    "        loss = F.nll_loss(output, target.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (idx+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, idx+1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    loss_list = []\n",
    "    acc_list = []\n",
    "    test_dataloader = get_dataloader(train = False,batch_size=TEST_BATCH_SIZE)\n",
    "    for idx,(input,target) in enumerate(test_dataloader):\n",
    "        with torch.no_grad():\n",
    "            output = model(input.to(device))\n",
    "            target = target.to(device)\n",
    "            cur_loss = F.nll_loss(output, target)\n",
    "            loss_list.append(cur_loss.cpu())\n",
    "            pred = output.max(dim = -1)[-1]\n",
    "            cur_acc = pred.eq(target).float().mean()\n",
    "            acc_list.append(cur_acc.cpu())\n",
    "    print(\"Mean accuracy: \", np.mean(acc_list), \"Mean loss: \", np.mean(loss_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9913344it [00:05, 1788335.66it/s]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29696it [00:00, 457880.59it/s]           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1649664it [00:01, 1543288.22it/s]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5120it [00:00, 2898088.59it/s]          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Mean accuracy:  0.0778 Mean loss:  2.3645055\n",
      "Epoch [1/3], Step [100/469], Loss: 0.4857\n",
      "Epoch [1/3], Step [200/469], Loss: 0.3060\n",
      "Epoch [1/3], Step [300/469], Loss: 0.1634\n",
      "Epoch [1/3], Step [400/469], Loss: 0.2794\n",
      "Epoch [2/3], Step [100/469], Loss: 0.0652\n",
      "Epoch [2/3], Step [200/469], Loss: 0.1567\n",
      "Epoch [2/3], Step [300/469], Loss: 0.1642\n",
      "Epoch [2/3], Step [400/469], Loss: 0.0900\n",
      "Epoch [3/3], Step [100/469], Loss: 0.0728\n",
      "Epoch [3/3], Step [200/469], Loss: 0.0645\n",
      "Epoch [3/3], Step [300/469], Loss: 0.0895\n",
      "Epoch [3/3], Step [400/469], Loss: 0.1474\n",
      "Mean accuracy:  0.9712001 Mean loss:  0.095482014\n"
     ]
    }
   ],
   "source": [
    "test()\n",
    "num_epochs = 3\n",
    "for i in range(num_epochs):\n",
    "    train(i, num_epochs)\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MnistCNN().to(device)\n",
    "optimizer = Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy:  0.060399998 Mean loss:  2.303498\n",
      "Epoch [1/3], Step [100/469], Loss: 0.1260\n",
      "Epoch [1/3], Step [200/469], Loss: 0.1176\n",
      "Epoch [1/3], Step [300/469], Loss: 0.0708\n",
      "Epoch [1/3], Step [400/469], Loss: 0.0134\n",
      "Epoch [2/3], Step [100/469], Loss: 0.0666\n",
      "Epoch [2/3], Step [200/469], Loss: 0.1009\n",
      "Epoch [2/3], Step [300/469], Loss: 0.0624\n",
      "Epoch [2/3], Step [400/469], Loss: 0.0390\n",
      "Epoch [3/3], Step [100/469], Loss: 0.0325\n",
      "Epoch [3/3], Step [200/469], Loss: 0.0217\n",
      "Epoch [3/3], Step [300/469], Loss: 0.0547\n",
      "Epoch [3/3], Step [400/469], Loss: 0.0479\n",
      "Mean accuracy:  0.9858001 Mean loss:  0.04219047\n"
     ]
    }
   ],
   "source": [
    "test()\n",
    "num_epochs = 3\n",
    "for i in range(num_epochs):\n",
    "    train(i, num_epochs)\n",
    "test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
