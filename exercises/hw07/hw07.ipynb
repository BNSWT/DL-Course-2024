{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 7 - Experiments on RNN and LSTM\n",
    "\n",
    "Please implement the following two functions:\n",
    "- MnistRNN() - Design a RNN\n",
    "- MnistLSTM() - Design a LSTM \n",
    "\n",
    "Please train two models on the Mnist dataset and print the training results for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/zhouyuyang/env/miniconda3/envs/dl/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import Compose,ToTensor,Normalize\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "TEST_BATCH_SIZE = 1000\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.backends.cudnn.enabled = False\n",
    "\n",
    "# dataloader for the dataset\n",
    "def get_dataloader(train,batch_size=BATCH_SIZE):\n",
    "    transform_fn = Compose([\n",
    "        ToTensor(),\n",
    "        Normalize(mean = (0.1307,), std = (0.3081,))\n",
    "        ]) \n",
    "    dataset = MNIST(root = './data',train = train,transform = transform_fn, download = True)\n",
    "    data_loader = DataLoader(dataset,batch_size = batch_size,shuffle = True)\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MnistRNN(nn.Module):\n",
    "    def __init__(self, input_dim=1, hidden_dim=128, output_dim=10):\n",
    "        super(MnistRNN, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(input_dim, hidden_dim, kernel_size=4, stride=4),\n",
    "            nn.ReLU())\n",
    "\n",
    "        # Define the RNN layer\n",
    "        self.rnn = nn.RNN(input_size=hidden_dim, hidden_size=hidden_dim, num_layers=1, batch_first=True)\n",
    "\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Stem network (convolutional + activation)\n",
    "        inputs = self.stem(inputs)\n",
    "        inputs = inputs.view(inputs.size(0), -1, inputs.size(1)) # B x seq_len x hidden_dim\n",
    "        h0 = torch.zeros(1, inputs.size(0), self.hidden_dim).to(inputs.device) # Initialize hidden state\n",
    "        out, _ = self.rnn(inputs, h0) # Pass through RNN layer\n",
    "        out = out[:, -1, :] # Take the last time step's output\n",
    "        output = self.softmax(out) # Apply softmax activation\n",
    "        return output\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MnistLSTM(nn.Module):\n",
    "    def __init__(self, input_dim=1, hidden_dim=128, output_dim=10):\n",
    "        super(MnistLSTM, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(input_dim, hidden_dim, kernel_size=4, stride=4),\n",
    "            nn.ReLU())\n",
    "\n",
    "        # Define the LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size=hidden_dim, hidden_size=hidden_dim, num_layers=1, batch_first=True)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inputs = self.stem(inputs)\n",
    "        inputs = inputs.view(inputs.size(0), -1, self.hidden_dim)  # B x (7*7) x D\n",
    "\n",
    "        # Forward through LSTM\n",
    "        lstm_output, (hidden, cell) = self.lstm(inputs)\n",
    "\n",
    "        # We can use the last hidden state as the output for classification\n",
    "        output = self.fc(hidden[-1])  # Get the last hidden state\n",
    "        output = self.softmax(output)\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MnistRNN().to(device)\n",
    "optimizer = Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, num_epochs):\n",
    "    data_loader = get_dataloader(True)\n",
    "    total_step = len(data_loader)\n",
    "    for idx, (input, target) in enumerate(data_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input.to(device))\n",
    "        loss = F.nll_loss(output, target.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (idx+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, idx+1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    loss_list = []\n",
    "    acc_list = []\n",
    "    test_dataloader = get_dataloader(train = False, batch_size=TEST_BATCH_SIZE)\n",
    "    for idx,(input,target) in enumerate(test_dataloader):\n",
    "        with torch.no_grad():\n",
    "            output = model(input.to(device))\n",
    "            target = target.to(device)\n",
    "            cur_loss = F.nll_loss(output, target)\n",
    "            loss_list.append(cur_loss.cpu())\n",
    "            pred = output.max(dim = -1)[-1]\n",
    "            cur_acc = pred.eq(target).float().mean()\n",
    "            acc_list.append(cur_acc.cpu())\n",
    "    print(\"Mean accuracy: \", np.mean(acc_list), \"Mean loss: \", np.mean(loss_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy:  0.0033999998 Mean loss:  4.8232107\n",
      "Epoch [1/3], Step [100/469], Loss: 3.2151\n",
      "Epoch [1/3], Step [200/469], Loss: 3.1464\n",
      "Epoch [1/3], Step [300/469], Loss: 3.1051\n",
      "Epoch [1/3], Step [400/469], Loss: 3.0577\n",
      "Epoch [2/3], Step [100/469], Loss: 3.0076\n",
      "Epoch [2/3], Step [200/469], Loss: 3.0137\n",
      "Epoch [2/3], Step [300/469], Loss: 2.9919\n",
      "Epoch [2/3], Step [400/469], Loss: 2.9549\n",
      "Epoch [3/3], Step [100/469], Loss: 2.9459\n",
      "Epoch [3/3], Step [200/469], Loss: 2.9602\n",
      "Epoch [3/3], Step [300/469], Loss: 2.9329\n",
      "Epoch [3/3], Step [400/469], Loss: 2.9428\n",
      "Mean accuracy:  0.93149996 Mean loss:  2.9467125\n"
     ]
    }
   ],
   "source": [
    "test()\n",
    "num_epochs = 3\n",
    "for i in range(num_epochs):\n",
    "    train(i, num_epochs)\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MnistLSTM().to(device)\n",
    "optimizer = Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy:  0.0759 Mean loss:  2.3054051\n",
      "Epoch [1/3], Step [100/469], Loss: 0.3155\n",
      "Epoch [1/3], Step [200/469], Loss: 0.3201\n",
      "Epoch [1/3], Step [300/469], Loss: 0.2750\n",
      "Epoch [1/3], Step [400/469], Loss: 0.1922\n",
      "Epoch [2/3], Step [100/469], Loss: 0.1175\n",
      "Epoch [2/3], Step [200/469], Loss: 0.1012\n",
      "Epoch [2/3], Step [300/469], Loss: 0.0957\n",
      "Epoch [2/3], Step [400/469], Loss: 0.1270\n",
      "Epoch [3/3], Step [100/469], Loss: 0.0656\n",
      "Epoch [3/3], Step [200/469], Loss: 0.0670\n",
      "Epoch [3/3], Step [300/469], Loss: 0.0827\n",
      "Epoch [3/3], Step [400/469], Loss: 0.0380\n",
      "Mean accuracy:  0.9739 Mean loss:  0.07997291\n"
     ]
    }
   ],
   "source": [
    "test()\n",
    "num_epochs = 3\n",
    "for i in range(num_epochs):\n",
    "    train(i, num_epochs)\n",
    "test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
